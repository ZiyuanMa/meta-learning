{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "root_path = './../'\n",
    "processed_folder =  os.path.join(root_path)\n",
    "\n",
    "zip_ref = zipfile.ZipFile(os.path.join(root_path,'omniglot.zip'), 'r')\n",
    "zip_ref.extractall(root_path)\n",
    "zip_ref.close()\n",
    "root_dir = './../omniglot/python'\n",
    "root_dir_train = os.path.join(root_dir,'images_background')"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T02:33:53.570525Z",
     "start_time": "2020-04-16T02:33:53.256842Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T06:55:03.418079Z",
     "start_time": "2020-04-07T06:55:03.346124Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 数据预处理\n",
    "拿到原始数据之后先将下面的代码取消注释，进行数据预处理。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "\n",
    "# # 数据预处理\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# '''\n",
    "# an example of img_items:\n",
    "# ( '0709_17.png',\n",
    "#   'Alphabet_of_the_Magi/character01',\n",
    "#   './../datasets/omniglot/python/images_background/Alphabet_of_the_Magi/character01')\n",
    "# '''\n",
    "\n",
    "\n",
    "root_dir_train = os.path.join(root_dir, 'images_background')\n",
    "root_dir_test = os.path.join(root_dir, 'images_evaluation')\n",
    "\n",
    "def find_classes(root_dir_train):\n",
    "    img_items = []\n",
    "    for (root, dirs, files) in os.walk(root_dir_train): \n",
    "        for file in files:\n",
    "            if (file.endswith(\"png\")):\n",
    "                r = root.split('/')\n",
    "                img_items.append((file, r[-2] + \"/\" + r[-1], root))\n",
    "    print(\"== Found %d items \" % len(img_items))\n",
    "    return img_items\n",
    "\n",
    "# ## 构建一个词典{class:idx}\n",
    "def index_classes(items):\n",
    "    class_idx = {}\n",
    "    count = 0\n",
    "    for item in items:\n",
    "        if item[1] not in class_idx:\n",
    "            class_idx[item[1]] = count\n",
    "            count += 1\n",
    "    print('== Found {} classes'.format(len(class_idx)))\n",
    "    return class_idx\n",
    "        \n",
    "\n",
    "img_items_train =  find_classes(root_dir_train) # [(file1, label1, root1),..]\n",
    "img_items_test = find_classes(root_dir_test)\n",
    "\n",
    "class_idx_train = index_classes(img_items_train)\n",
    "class_idx_test = index_classes(img_items_test)\n",
    "\n",
    "\n",
    "def generate_temp(img_items,class_idx):\n",
    "    temp = dict()\n",
    "    for imgname, classes, dirs in img_items:\n",
    "        img = '{}/{}'.format(dirs, imgname)\n",
    "        label = class_idx[classes]\n",
    "        transform = transforms.Compose([lambda img: Image.open(img).convert('L'),\n",
    "                                  lambda img: img.resize((28,28)),\n",
    "                                  lambda img: np.reshape(img, (28,28,1)),\n",
    "                                  lambda img: np.transpose(img, [2,0,1]),\n",
    "                                  lambda img: img/255.\n",
    "                                  ])\n",
    "        img = transform(img)\n",
    "        if label in temp.keys():\n",
    "            temp[label].append(img)\n",
    "        else:\n",
    "            temp[label] = [img]\n",
    "    print('begin to generate omniglot.npy')\n",
    "    return temp\n",
    "#     ## 每个字符包含20个样本\n",
    "\n",
    "temp_train = generate_temp(img_items_train, class_idx_train)\n",
    "temp_test = generate_temp(img_items_test, class_idx_test)\n",
    "\n",
    "img_list = []\n",
    "for label, imgs in temp_train.items():\n",
    "    img_list.append(np.array(imgs))\n",
    "img_list = np.array(img_list).astype(np.float) # [[20 imgs],..., 1623 classes in total]\n",
    "print('data shape:{}'.format(img_list.shape)) # (964, 20, 1, 28, 28)\n",
    "np.save(os.path.join(root_dir, 'omniglot_train.npy'), img_list)\n",
    "print('end.')\n",
    "\n",
    "\n",
    "img_list = []\n",
    "for label, imgs in temp_test.items():\n",
    "    img_list.append(np.array(imgs))\n",
    "img_list = np.array(img_list).astype(np.float) # [[20 imgs],..., 1623 classes in total]\n",
    "print('data shape:{}'.format(img_list.shape)) # (659, 20, 1, 28, 28)\n",
    "\n",
    "np.save(os.path.join(root_dir, 'omniglot_test.npy'), img_list)\n",
    "print('end.')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "== Found 19280 items \n",
      "== Found 13180 items \n",
      "== Found 964 classes\n",
      "== Found 659 classes\n",
      "begin to generate omniglot.npy\n",
      "begin to generate omniglot.npy\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/var/folders/zh/wcyb01nj18xfqrrty4rgp_nm0000gn/T/ipykernel_3280/1289281119.py:71: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  img_list = np.array(img_list).astype(np.float) # [[20 imgs],..., 1623 classes in total]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "data shape:(964, 20, 1, 28, 28)\n",
      "end.\n",
      "data shape:(659, 20, 1, 28, 28)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/var/folders/zh/wcyb01nj18xfqrrty4rgp_nm0000gn/T/ipykernel_3280/1289281119.py:80: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  img_list = np.array(img_list).astype(np.float) # [[20 imgs],..., 1623 classes in total]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "end.\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T02:33:55.423210Z",
     "start_time": "2020-04-16T02:33:55.405143Z"
    },
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "img_list_train = np.load(os.path.join(root_dir, 'omniglot_train.npy')) # (964, 20, 1, 28, 28)\n",
    "img_list_test = np.load(os.path.join(root_dir, 'omniglot_test.npy')) # (659, 20, 1, 28, 28)\n",
    "\n",
    "x_train = img_list_train\n",
    "x_test = img_list_test\n",
    "# num_classes = img_list.shape[0]\n",
    "datasets = {'train': x_train, 'test': x_test}"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T02:34:05.401785Z",
     "start_time": "2020-04-16T02:34:03.474067Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "### 准备数据迭代器\n",
    "n_way = 20\n",
    "k_spt = 5  ## support data 的个数\n",
    "k_query = 15 ## query data 的个数\n",
    "imgsz = 28\n",
    "resize = imgsz\n",
    "task_num = 16\n",
    "batch_size = task_num\n",
    "\n",
    "indexes = {\"train\": 0, \"test\": 0}\n",
    "datasets = {\"train\": x_train, \"test\": x_test}\n",
    "print(\"DB: train\", x_train.shape, \"test\", x_test.shape)\n",
    "\n",
    "\n",
    "def load_data_cache(dataset):\n",
    "    \"\"\"\n",
    "    Collects several batches data for N-shot learning\n",
    "    :param dataset: [cls_num, 20, 84, 84, 1]\n",
    "    :return: A list with [support_set_x, support_set_y, target_x, target_y] ready to be fed to our networks\n",
    "    \"\"\"\n",
    "    #  take 5 way 1 shot as example: 5 * 1\n",
    "    setsz = k_spt * n_way\n",
    "    querysz = k_query * n_way\n",
    "    data_cache = []\n",
    "\n",
    "    # print('preload next 10 caches of batch_size of batch.')\n",
    "    for sample in range(10):  # num of epochs\n",
    "\n",
    "        x_spts, y_spts, x_qrys, y_qrys = [], [], [], []\n",
    "        for i in range(batch_size):  # one batch means one set\n",
    "\n",
    "            x_spt, y_spt, x_qry, y_qry = [], [], [], []\n",
    "            selected_cls = np.random.choice(dataset.shape[0], n_way, replace =  False) \n",
    "\n",
    "            for j, cur_class in enumerate(selected_cls):\n",
    "\n",
    "                selected_img = np.random.choice(20, k_spt + k_query, replace = False)\n",
    "\n",
    "                # 构造support集和query集\n",
    "                x_spt.append(dataset[cur_class][selected_img[:k_spt]])\n",
    "                x_qry.append(dataset[cur_class][selected_img[k_spt:]])\n",
    "                y_spt.append([j for _ in range(k_spt)])\n",
    "                y_qry.append([j for _ in range(k_query)])\n",
    "\n",
    "            # shuffle inside a batch\n",
    "            perm = np.random.permutation(n_way * k_spt)\n",
    "            x_spt = np.array(x_spt).reshape(n_way * k_spt, 1, resize, resize)[perm]\n",
    "            y_spt = np.array(y_spt).reshape(n_way * k_spt)[perm]\n",
    "            perm = np.random.permutation(n_way * k_query)\n",
    "            x_qry = np.array(x_qry).reshape(n_way * k_query, 1, resize, resize)[perm]\n",
    "            y_qry = np.array(y_qry).reshape(n_way * k_query)[perm]\n",
    " \n",
    "            # append [sptsz, 1, 84, 84] => [batch_size, setsz, 1, 84, 84]\n",
    "            x_spts.append(x_spt)\n",
    "            y_spts.append(y_spt)\n",
    "            x_qrys.append(x_qry)\n",
    "            y_qrys.append(y_qry)\n",
    "\n",
    "#         print(x_spts[0].shape)\n",
    "        # [b, setsz = n_way * k_spt, 1, 84, 84]\n",
    "        x_spts = np.array(x_spts).astype(np.float32).reshape(batch_size, setsz, 1, resize, resize)\n",
    "        y_spts = np.array(y_spts).astype(np.int).reshape(batch_size, setsz)\n",
    "        # [b, qrysz = n_way * k_query, 1, 84, 84]\n",
    "        x_qrys = np.array(x_qrys).astype(np.float32).reshape(batch_size, querysz, 1, resize, resize)\n",
    "        y_qrys = np.array(y_qrys).astype(np.int).reshape(batch_size, querysz)\n",
    "#         print(x_qrys.shape)\n",
    "        data_cache.append([x_spts, y_spts, x_qrys, y_qrys])\n",
    "\n",
    "    return data_cache\n",
    "\n",
    "datasets_cache = {\"train\": load_data_cache(x_train),  # current epoch data cached\n",
    "                       \"test\": load_data_cache(x_test)}\n",
    "\n",
    "def next(mode='train'):\n",
    "    \"\"\"\n",
    "    Gets next batch from the dataset with name.\n",
    "    :param mode: The name of the splitting (one of \"train\", \"val\", \"test\")\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # update cache if indexes is larger than len(data_cache)\n",
    "    if indexes[mode] >= len(datasets_cache[mode]):\n",
    "        indexes[mode] = 0\n",
    "        datasets_cache[mode] = load_data_cache(datasets[mode])\n",
    "\n",
    "    next_batch = datasets_cache[mode][indexes[mode]]\n",
    "    indexes[mode] += 1\n",
    "\n",
    "    return next_batch\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DB: train (964, 20, 1, 28, 28) test (659, 20, 1, 28, 28)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/var/folders/zh/wcyb01nj18xfqrrty4rgp_nm0000gn/T/ipykernel_3280/3887987129.py:62: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y_spts = np.array(y_spts).astype(np.int).reshape(batch_size, setsz)\n",
      "/var/folders/zh/wcyb01nj18xfqrrty4rgp_nm0000gn/T/ipykernel_3280/3887987129.py:65: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y_qrys = np.array(y_qrys).astype(np.int).reshape(batch_size, querysz)\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T02:34:22.871343Z",
     "start_time": "2020-04-16T02:34:21.674782Z"
    },
    "code_folding": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from copy import deepcopy,copy\n",
    "        \n",
    "\n",
    "class BaseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseNet, self).__init__()\n",
    "        self.vars = nn.ParameterList()  ## 包含了所有需要被优化的tensor\n",
    "        self.vars_bn = nn.ParameterList()\n",
    "        \n",
    "        # 第1个conv2d\n",
    "        weight = nn.Parameter(torch.ones(64, 1, 3, 3))\n",
    "        nn.init.kaiming_normal_(weight)\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight,bias])\n",
    "        \n",
    "        # 第1个BatchNorm层\n",
    "        weight = nn.Parameter(torch.ones(64))\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight,bias])\n",
    "        \n",
    "        running_mean = nn.Parameter(torch.zeros(64), requires_grad= False)\n",
    "        running_var = nn.Parameter(torch.zeros(64), requires_grad= False)\n",
    "        self.vars_bn.extend([running_mean, running_var])\n",
    "        \n",
    "        # 第2个conv2d\n",
    "        weight = nn.Parameter(torch.ones(64, 64, 3, 3))\n",
    "        nn.init.kaiming_normal_(weight)\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight,bias])\n",
    "        \n",
    "        # 第2个BatchNorm层\n",
    "        weight = nn.Parameter(torch.ones(64))\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight,bias])\n",
    "        \n",
    "        running_mean = nn.Parameter(torch.zeros(64), requires_grad= False)\n",
    "        running_var = nn.Parameter(torch.zeros(64), requires_grad= False)\n",
    "        self.vars_bn.extend([running_mean, running_var])\n",
    "        \n",
    "        # 第3个conv2d\n",
    "        weight = nn.Parameter(torch.ones(64, 64, 3, 3))\n",
    "        nn.init.kaiming_normal_(weight)\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight,bias])\n",
    "        \n",
    "        # 第3个BatchNorm层\n",
    "        weight = nn.Parameter(torch.ones(64))\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight,bias])\n",
    "        \n",
    "        running_mean = nn.Parameter(torch.zeros(64), requires_grad= False)\n",
    "        running_var = nn.Parameter(torch.zeros(64), requires_grad= False)\n",
    "        self.vars_bn.extend([running_mean, running_var])\n",
    "        \n",
    "        # 第4个conv2d\n",
    "        weight = nn.Parameter(torch.ones(64, 64, 3, 3))\n",
    "        nn.init.kaiming_normal_(weight)\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight,bias])\n",
    "        \n",
    "        # 第4个BatchNorm层\n",
    "        weight = nn.Parameter(torch.ones(64))\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight,bias])\n",
    "        \n",
    "        running_mean = nn.Parameter(torch.zeros(64), requires_grad= False)\n",
    "        running_var = nn.Parameter(torch.zeros(64), requires_grad= False)\n",
    "        self.vars_bn.extend([running_mean, running_var])\n",
    "        \n",
    "        ##linear\n",
    "        weight = nn.Parameter(torch.ones([20,64]))\n",
    "        bias = nn.Parameter(torch.zeros(20))\n",
    "        self.vars.extend([weight,bias])\n",
    "        \n",
    "    def forward(self, x, params = None, bn_training=True):\n",
    "        '''\n",
    "        :bn_training: set False to not update\n",
    "        :return: \n",
    "        '''\n",
    "        if params is None:\n",
    "            params = self.vars\n",
    "        \n",
    "        weight, bias = params[0], params[1]  # 第1个CONV层\n",
    "        x = F.conv2d(x, weight, bias, stride = 1, padding = 1)\n",
    "        weight, bias = params[2], params[3]  # 第1个BN层\n",
    "        running_mean, running_var = self.vars_bn[0], self.vars_bn[1]\n",
    "        x = F.batch_norm(x, running_mean, running_var, weight=weight,bias =bias, training= bn_training, momentum = 1)\n",
    "        x = F.relu(x, inplace = [True])  #第1个relu\n",
    "        x = F.max_pool2d(x,kernel_size=2)  #第1个MAX_POOL层  \n",
    "        \n",
    "        \n",
    "        \n",
    "        weight, bias = params[4], params[5]  # 第2个CONV层\n",
    "        x = F.conv2d(x, weight, bias, stride = 1, padding = 1)\n",
    "        weight, bias = params[6], params[7]  # 第2个BN层\n",
    "        running_mean, running_var = self.vars_bn[2], self.vars_bn[3]\n",
    "        x = F.batch_norm(x, running_mean, running_var, weight=weight,bias =bias, training= bn_training, momentum=1)\n",
    "        x = F.relu(x, inplace = [True])  #第2个relu\n",
    "        x = F.max_pool2d(x,kernel_size=2)  #第2个MAX_POOL层   \n",
    "        \n",
    "        \n",
    "        weight, bias = params[8], params[9]  # 第3个CONV层\n",
    "        x = F.conv2d(x, weight, bias, stride = 1, padding = 1)\n",
    "        weight, bias = params[10], params[11]  # 第3个BN层\n",
    "        running_mean, running_var = self.vars_bn[4], self.vars_bn[5]\n",
    "        x = F.batch_norm(x, running_mean, running_var, weight=weight,bias =bias, training= bn_training,momentum=1)\n",
    "        x = F.relu(x, inplace = [True])  #第3个relu,\n",
    "        x = F.max_pool2d(x,kernel_size=2)  #第3个MAX_POOL层\n",
    "        \n",
    "        \n",
    "        weight, bias = params[12], params[13]  # 第4个CONV层\n",
    "        x = F.conv2d(x, weight, bias, stride = 1, padding = 1)\n",
    "        weight, bias = params[14], params[15]  # 第4个BN层\n",
    "        running_mean, running_var = self.vars_bn[6], self.vars_bn[7]\n",
    "        x = F.batch_norm(x, running_mean, running_var, weight=weight,bias =bias, training= bn_training)\n",
    "        x = F.max_pool2d(x,kernel_size=2)  #第4个MAX_POOL层\n",
    "        \n",
    "        x = F.relu(x, inplace = [True])  #第4个relu\n",
    "        \n",
    "        x = x.view(x.size(0), -1) ## flatten\n",
    "        weight, bias = params[-2], params[-1]  # linear\n",
    "        x = F.linear(x, weight, bias)\n",
    "        \n",
    "        output = x\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def parameters(self):\n",
    "        \n",
    "        return self.vars\n"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T02:34:25.602642Z",
     "start_time": "2020-04-16T02:34:25.547982Z"
    },
    "code_folding": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-29T12:00:30.197710Z",
     "start_time": "2020-02-29T12:00:30.186076Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-29T05:41:40.773998Z",
     "start_time": "2020-02-29T05:41:40.762077Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "class MetaLearner(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MetaLearner, self).__init__()\n",
    "        self.update_step = 5 ## task-level inner update steps\n",
    "        self.update_step_test = 5\n",
    "        self.net = BaseNet()\n",
    "        self.meta_lr = 0.0008\n",
    "        self.base_lr = 0.075\n",
    "        self.meta_optim = torch.optim.Adam(self.net.parameters(), lr = self.meta_lr)\n",
    "#         self.meta_optim = torch.optim.SGD(self.net.parameters(), lr = self.meta_lr, momentum = 0.9, weight_decay=0.0005)\n",
    "        \n",
    "    def forward(self,x_spt, y_spt, x_qry, y_qry):\n",
    "        # 初始化\n",
    "        task_num, ways, shots, h, w = x_spt.size()\n",
    "        query_size = x_qry.size(1) # 75 = 15 * 5\n",
    "        loss_list_qry = [0 for _ in range(self.update_step + 1)]\n",
    "        correct_list = [0 for _ in range(self.update_step + 1)]\n",
    "        \n",
    "        for i in range(task_num):\n",
    "            ## 第0步更新\n",
    "            y_hat = self.net(x_spt[i], params=None, bn_training=True) # (ways * shots, ways)\n",
    "            loss = F.cross_entropy(y_hat, y_spt[i]) \n",
    "            grad = torch.autograd.grad(loss, self.net.parameters())\n",
    "            tuples = zip(grad, self.net.parameters()) ## 将梯度和参数\\theta一一对应起来\n",
    "            # fast_weights这一步相当于求了一个\\theta - \\alpha*\\nabla(L)\n",
    "            fast_weights = list(map(lambda p: p[1] - self.base_lr * p[0], tuples))\n",
    "            # 在query集上测试，计算准确率\n",
    "            # 这一步使用更新前的数据\n",
    "            with torch.no_grad():\n",
    "                y_hat = self.net(x_qry[i], self.net.parameters(), bn_training=True)\n",
    "                loss_qry = F.cross_entropy(y_hat, y_qry[i])\n",
    "                loss_list_qry[0] += loss_qry\n",
    "                pred_qry = F.softmax(y_hat, dim=1).argmax(dim=1)  # size = (75)\n",
    "                correct = torch.eq(pred_qry, y_qry[i]).sum().item()\n",
    "                correct_list[0] += correct\n",
    "            \n",
    "            # 使用更新后的数据在query集上测试。\n",
    "            with torch.no_grad():\n",
    "                y_hat = self.net(x_qry[i], fast_weights, bn_training = True)\n",
    "                loss_qry = F.cross_entropy(y_hat, y_qry[i])\n",
    "                loss_list_qry[1] += loss_qry\n",
    "                pred_qry = F.softmax(y_hat, dim=1).argmax(dim=1)  # size = (75)\n",
    "                correct = torch.eq(pred_qry, y_qry[i]).sum().item()\n",
    "                correct_list[1] += correct   \n",
    "            \n",
    "            for k in range(1, self.update_step):\n",
    "                \n",
    "                y_hat = self.net(x_spt[i], params = fast_weights, bn_training=True)\n",
    "                loss = F.cross_entropy(y_hat, y_spt[i])\n",
    "                grad = torch.autograd.grad(loss, fast_weights)\n",
    "                tuples = zip(grad, fast_weights) \n",
    "                fast_weights = list(map(lambda p: p[1] - self.base_lr * p[0], tuples))\n",
    "                \n",
    "                if k < self.update_step - 1:\n",
    "                    with torch.no_grad():\n",
    "                        y_hat = self.net(x_qry[i], params = fast_weights, bn_training = True)\n",
    "                        loss_qry = F.cross_entropy(y_hat, y_qry[i])\n",
    "                        loss_list_qry[k+1] += loss_qry\n",
    "                else:\n",
    "                    y_hat = self.net(x_qry[i], params = fast_weights, bn_training = True)\n",
    "                    loss_qry = F.cross_entropy(y_hat, y_qry[i])\n",
    "                    loss_list_qry[k+1] += loss_qry.detach()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    pred_qry = F.softmax(y_hat,dim=1).argmax(dim=1)\n",
    "                    correct = torch.eq(pred_qry, y_qry[i]).sum().item()\n",
    "                    correct_list[k+1] += correct\n",
    "\n",
    "                \n",
    "        loss_qry = loss_list_qry[-1] / task_num\n",
    "        self.meta_optim.zero_grad() # 梯度清零\n",
    "        loss_qry.backward()\n",
    "        self.meta_optim.step()\n",
    "        \n",
    "        accs = np.array(correct_list) / (query_size * task_num)\n",
    "        loss = np.array(loss_list_qry) / task_num\n",
    "\n",
    "        return accs, loss\n",
    "\n",
    "    \n",
    "    \n",
    "    def finetunning(self, x_spt, y_spt, x_qry, y_qry):\n",
    "        assert len(x_spt.shape) == 4\n",
    "        \n",
    "        query_size = x_qry.size(0)\n",
    "        correct_list = [0 for _ in range(self.update_step_test + 1)]\n",
    "        \n",
    "        new_net = deepcopy(self.net)\n",
    "        y_hat = new_net(x_spt)\n",
    "        loss = F.cross_entropy(y_hat, y_spt)\n",
    "        grad = torch.autograd.grad(loss, new_net.parameters())\n",
    "        fast_weights = list(map(lambda p:p[1] - self.base_lr * p[0], zip(grad, new_net.parameters())))\n",
    "        \n",
    "        # 在query集上测试，计算准确率\n",
    "        # 这一步使用更新前的数据\n",
    "        with torch.no_grad():\n",
    "            y_hat = new_net(x_qry,  params = new_net.parameters(), bn_training = True)\n",
    "            pred_qry = F.softmax(y_hat, dim=1).argmax(dim=1)  # size = (75)\n",
    "            correct = torch.eq(pred_qry, y_qry).sum().item()\n",
    "            correct_list[0] += correct\n",
    "\n",
    "        # 使用更新后的数据在query集上测试。\n",
    "        with torch.no_grad():\n",
    "            y_hat = new_net(x_qry, params = fast_weights, bn_training = True)\n",
    "            pred_qry = F.softmax(y_hat, dim=1).argmax(dim=1)  # size = (75)\n",
    "            correct = torch.eq(pred_qry, y_qry).sum().item()\n",
    "            correct_list[1] += correct\n",
    "\n",
    "        for k in range(1, self.update_step_test):\n",
    "            y_hat = new_net(x_spt, params = fast_weights, bn_training=True)\n",
    "            loss = F.cross_entropy(y_hat, y_spt)\n",
    "            grad = torch.autograd.grad(loss, fast_weights)\n",
    "            fast_weights = list(map(lambda p:p[1] - self.base_lr * p[0], zip(grad, fast_weights)))\n",
    "            \n",
    "            y_hat = new_net(x_qry, fast_weights, bn_training=True)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                pred_qry = F.softmax(y_hat, dim=1).argmax(dim=1)\n",
    "                correct = torch.eq(pred_qry, y_qry).sum().item()\n",
    "                correct_list[k+1] += correct\n",
    "                \n",
    "        del new_net\n",
    "        accs = np.array(correct_list) / query_size\n",
    "        return accs\n",
    "        "
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T03:02:31.026926Z",
     "start_time": "2020-04-17T03:02:30.989680Z"
    },
    "code_folding": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# net = torch.load('./trained_models/MTL-5000epochs.pt')"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T03:02:38.654198Z",
     "start_time": "2020-04-17T03:02:38.651549Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "## omniglot\n",
    "import random\n",
    "random.seed(1337)\n",
    "np.random.seed(1337)\n",
    "\n",
    "import time\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "meta = MetaLearner().to(device)\n",
    "\n",
    "epochs = 60001\n",
    "for step in range(epochs):\n",
    "    start = time.time()\n",
    "    x_spt, y_spt, x_qry, y_qry = next('train')\n",
    "    x_spt, y_spt, x_qry, y_qry = torch.from_numpy(x_spt).to(device),\\\n",
    "                                 torch.from_numpy(y_spt).to(device),\\\n",
    "                                 torch.from_numpy(x_qry).to(device),\\\n",
    "                                 torch.from_numpy(y_qry).to(device)\n",
    "    accs,loss = meta(x_spt, y_spt, x_qry, y_qry)\n",
    "    end = time.time()\n",
    "    if step % 100 == 0:\n",
    "        print(\"epoch:\" ,step)\n",
    "        print(accs)\n",
    "#         print(loss)\n",
    "        \n",
    "    if step % 1000 == 0:\n",
    "        accs = []\n",
    "        for _ in range(1000//task_num):\n",
    "            # db_train.next('test')\n",
    "            x_spt, y_spt, x_qry, y_qry = next('test')\n",
    "            x_spt, y_spt, x_qry, y_qry = torch.from_numpy(x_spt).to(device),\\\n",
    "                                         torch.from_numpy(y_spt).to(device),\\\n",
    "                                         torch.from_numpy(x_qry).to(device),\\\n",
    "                                         torch.from_numpy(y_qry).to(device)\n",
    "\n",
    "            \n",
    "            for x_spt_one, y_spt_one, x_qry_one, y_qry_one in zip(x_spt, y_spt, x_qry, y_qry):\n",
    "                test_acc = meta.finetunning(x_spt_one, y_spt_one, x_qry_one, y_qry_one)\n",
    "                accs.append(test_acc)\n",
    "        print('在mean process之前：',np.array(accs).shape)\n",
    "        accs = np.array(accs).mean(axis=0).astype(np.float16)\n",
    "        print('测试集准确率:',accs)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/zh/wcyb01nj18xfqrrty4rgp_nm0000gn/T/ipykernel_3280/4102875522.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m                                  \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_qry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                                  \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_qry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0maccs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_spt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_spt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_qry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_qry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/zh/wcyb01nj18xfqrrty4rgp_nm0000gn/T/ipykernel_3280/377982823.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_spt, y_spt, x_qry, y_qry)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mloss_qry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_list_qry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtask_num\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 梯度清零\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mloss_qry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T12:33:25.158244Z",
     "start_time": "2020-04-17T03:03:26.192147Z"
    },
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T03:00:56.266331Z",
     "start_time": "2020-03-01T03:00:56.205955Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.9 64-bit"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "language_info": {
   "name": "python",
   "version": "3.8.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}